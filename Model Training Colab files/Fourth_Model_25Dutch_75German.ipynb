{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning a XLSR-Wav2Vec2 model for Word slicing using the Common Voice data\n"
      ],
      "metadata": {
        "id": "M_PyqzRuEtl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR) that can be fine tuned using unlabeled audio data to fine tune it for specialized tasks or to use less data.\n",
        "In this notebook I will use this model to train a Dutch base model to use for my thesis on research on the amount of bilangual data needed to perform well on bilangual tasks."
      ],
      "metadata": {
        "id": "WvrJg-gGFDTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start, let's install both datasets and transformers from master. Also, we need the torchaudio and librosa package to load audio files"
      ],
      "metadata": {
        "id": "hIlNdUu1GD3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4JX74izF-wE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets torchaudio jiwer librosa evaluate numpy\n",
        "!pip install transformers==4.11.13"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, e.g. a feature vector, and a tokenizer that processes the model's output format to text.\n",
        "\n",
        "In Transformers, the XLSR-Wav2Vec2 model is thus accompanied by both a tokenizer, called Wav2Vec2CTCTokenizer, and a feature extractor, called Wav2Vec2FeatureExtractor.\n",
        "\n",
        "Let's start by creating the tokenizer responsible for decoding the model's predictions."
      ],
      "metadata": {
        "id": "njDLUdS6GNEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wav2VecCTCTokenizer\n"
      ],
      "metadata": {
        "id": "fsqRxyJtGW6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2Vec2CTCTokenizer is a tokenizer used in automatic speech recognition (ASR) models, particularly with wav2vec 2.0 models trained using Connectionist Temporal Classification (CTC) loss. It is part of the Hugging Face transformers library and is designed to convert raw audio waveforms into text transcriptions.\n",
        "\n",
        " How It Works\n",
        "\n",
        "*   The Wav2Vec2CTCTokenizer is responsible for converting text into tokenized form and vice versa.\n",
        "*   It typically uses a character-level vocabulary, meaning each character in the target language (e.g., English letters, punctuation, spaces) has a corresponding token.\n",
        "*   Since CTC-based models do not produce word boundaries we'll need for word segmentation directly, the tokenizer helps reconstruct the text from raw predictions.\n",
        "\n",
        "\n",
        "\n",
        "Key Features\n",
        "\n",
        "1.   Character-Based Tokenization\n",
        "  *   Unlike word-based or subword-based tokenization (like BERT's WordPiece), it treats each character as a separate token.\n",
        "2.   CTC Blank Token (<pad> or |)\n",
        "  * A special token is used for handling alignment gaps in speech-to-text predictions.\n",
        "3. Lowercasing and Normalization\n",
        "  * Often used with lowercase-only text and minimal normalization (removing special characters).\n",
        "4. Vocabulary Handling\n",
        "  * Requires a vocab.json file specifying token-to-index mappings.\n"
      ],
      "metadata": {
        "id": "qYIH2oVmGkDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this specific notebook I will use the Dutch language, we will use both the train split, to train our data as well as the test split to validate the model.\n",
        "We will also remove all field that are not our audio files ('audio'), the file location ('path') and the textual representaion for fine-tuning ('sentence').\n",
        "To avoid catastrophic forgetting, we will merge & format both language data's together!"
      ],
      "metadata": {
        "id": "7xEwOR4KIfwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import evaluate\n",
        "import numpy as np\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "\n",
        "# Load a limited subset directly instead of streaming to avoid RAM issues\n",
        "common_voice_train_de = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"de\", split=\"train\", trust_remote_code=True).shuffle(seed=42).select(range(25000))\n",
        "common_voice_train_nl = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"nl\", split=\"train\", trust_remote_code=True).shuffle(seed=42)\n",
        "\n",
        "common_voice_test_de = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"de\", split=\"test\", trust_remote_code=True).shuffle(seed=42).select(range(10000))\n",
        "common_voice_test_nl = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"nl\", split=\"test\", trust_remote_code=True).shuffle(seed=42)\n",
        "\n",
        "# Get dataset sizes\n",
        "german_size = len(common_voice_train_de)\n",
        "dutch_size = len(common_voice_train_nl)\n",
        "german_test_size = len(common_voice_test_de)\n",
        "dutch_test_size = len(common_voice_test_nl)\n",
        "\n",
        "# Set total dataset size to match the smaller Dutch dataset\n",
        "total_size = dutch_size\n",
        "total_test_size = dutch_test_size\n",
        "\n",
        "# Compute correct sample sizes for 50% German, 50% Dutch\n",
        "german_sample_size = int(total_size * 0.75)\n",
        "dutch_sample_size = int(total_size * 0.25)\n",
        "german_test_sample_size = int(total_test_size * 0.75)\n",
        "dutch_test_sample_size = int(total_test_size * 0.25)\n",
        "\n",
        "# Sample correctly without converting to lists\n",
        "german_sample = common_voice_train_de.shuffle(seed=42).select(range(german_size))\n",
        "dutch_sample = common_voice_train_nl.shuffle(seed=42).select(range(dutch_sample_size))\n",
        "german_test_sample = common_voice_test_de.shuffle(seed=42).select(range(german_test_sample_size))\n",
        "dutch_test_sample = common_voice_test_nl.shuffle(seed=42).select(range(dutch_test_sample_size))\n",
        "\n",
        "# Merge and shuffle\n",
        "import random\n",
        "# Convert datasets to lists first\n",
        "mixed_dataset = german_sample.to_list() + dutch_sample.to_list()\n",
        "random.shuffle(mixed_dataset)\n",
        "\n",
        "# Convert datasets to lists first\n",
        "mixed_test_dataset = german_test_sample.to_list() + dutch_test_sample.to_list()\n",
        "random.shuffle(mixed_test_dataset)\n",
        "\n",
        "# Convert back to Dataset format\n",
        "from datasets import Dataset\n",
        "#common_voice_train = Dataset.from_list(mixed_dataset)\n",
        "#common_voice_test = Dataset.from_list(mixed_test_dataset)\n",
        "common_voice_train = common_voice_train_de\n",
        "common_voice_test = common_voice_test_de\n",
        "\n",
        "print(f\"Final train dataset size: {len(common_voice_train)} (75% German, 25% Dutch)\")\n",
        "print(f\"Final test dataset size: {len(common_voice_test)} (75% German, 25% Dutch)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jmWu8VPpH_rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
        "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
      ],
      "metadata": {
        "id": "PGCuKWuriXcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will give you an idea if the data is properly downloaded, it should display 10 random sentences in your database."
      ],
      "metadata": {
        "id": "HAkM3GwFJf9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))\n"
      ],
      "metadata": {
        "id": "36Yq3EZ9GBlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(common_voice_train.remove_columns([\"path\", \"audio\"]), num_examples=10)"
      ],
      "metadata": {
        "id": "ZkSPf7rAdW0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this looks pretty good, lots of these signs (, ? ! {} % etc.) dont mean much to our model, as the dont correspond to a characteristic sound unit, meaning we can't find a komma back in an audio file. We will also lowercase everything, as again, upper case letters mean very little to our model, but if we leave them in, the model will differentiate between them."
      ],
      "metadata": {
        "id": "vglF7-cSJ0ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "chars_to_ignore_regex = r'[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�€—]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
        "    return batch"
      ],
      "metadata": {
        "id": "j-oy96wLGH0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_train = common_voice_train.map(remove_special_characters)\n",
        "common_voice_test = common_voice_test.map(remove_special_characters)\n"
      ],
      "metadata": {
        "id": "WVl4T-y5dbwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(common_voice_train.remove_columns([\"path\",\"audio\"]))"
      ],
      "metadata": {
        "id": "7SPt5K5cddSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will make a lot more sense to our model.\n",
        "In CTC, it is common to classify speech chunks into letters, so we will do the same here. Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n",
        "\n",
        "We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. It is important to pass the argument batched=True to the map(...) function so that the mapping function has access to all transcriptions at once."
      ],
      "metadata": {
        "id": "auco9atcKiE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"sentence\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
      ],
      "metadata": {
        "id": "Crx_1IP9GIeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
        "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)\n"
      ],
      "metadata": {
        "id": "zI-7cEYzGKh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
      ],
      "metadata": {
        "id": "miNvGRdFdje8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict"
      ],
      "metadata": {
        "id": "K49_JElMdkvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to make it clearer that \" \", or the lack of a character, has its own token class, we give it a more visible character |. In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Common Voice's training set.\n",
        "\n",
        "Finally, we also add a padding token that corresponds to CTC's \"blank token\". The \"blank token\" is a core component of the CTC algorithm."
      ],
      "metadata": {
        "id": "lbZjXe9dLhoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]"
      ],
      "metadata": {
        "id": "uKLnutq7dmSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ],
      "metadata": {
        "id": "1Y_ajl4Qdo1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, now our vocabulary is complete and consists of 59 tokens, which means that the linear layer that we will add on top of the pretrained XLSR-Wav2Vec2 checkpoint will have an output dimension of 59.\n",
        "Next we will save it as a json file, as we can use this same vocabulary to train other models."
      ],
      "metadata": {
        "id": "TRZL7xDzLu_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ],
      "metadata": {
        "id": "Ut5eM902drXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a final step, we use the json file to instantiate an object of the Wav2Vec2CTCTokenizer class."
      ],
      "metadata": {
        "id": "M6bb3kZKN3xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
      ],
      "metadata": {
        "id": "g_1Zs84Pdt8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLSR-Wav2Vec2 Feature Extractor"
      ],
      "metadata": {
        "id": "pBpPtKXPN5OI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called sampling. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the real speech signal but also necessitates more values per second.\n",
        "\n",
        "A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, e.g., doubling the sampling rate results in data points being twice as long. Thus, before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n",
        "\n",
        "XLSR-Wav2Vec2 was pretrained on the audio data of Babel, Multilingual LibriSpeech (MLS), and Common Voice. Most of those datasets were sampled at 16kHz, so that Common Voice, sampled at 48kHz, has to be downsampled to 16kHz for training. Therefore, we will have to downsample our fine-tuning data to 16kHz in the following."
      ],
      "metadata": {
        "id": "6kbjbgAdODWV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUbPW7oV-B5"
      },
      "source": [
        "A XLSR-Wav2Vec2 feature extractor object requires the following parameters to be instantiated:\n",
        "\n",
        "- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n",
        "- `sampling_rate`: The sampling rate at which the model is trained on.\n",
        "- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n",
        "- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n",
        "- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLSR-Wav2Vec2 models should **always** make use of the `attention_mask`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
      ],
      "metadata": {
        "id": "Zwdyf76fduZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, XLSR-Wav2Vec2's feature extraction pipeline is thereby fully defined!\n",
        "\n",
        "To make the usage of XLSR-Wav2Vec2 as user-friendly as possible, the feature extractor and tokenizer are wrapped into a single Wav2Vec2Processor class so that one only needs a model and processor object."
      ],
      "metadata": {
        "id": "uijkqyohOiMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "zx3tLEFHd2TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we save the just created processor & the fine-tuned model to our google drive so we can use it to (further) train other models."
      ],
      "metadata": {
        "id": "RxOAi1SsOpQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "_uZ1d3Rpd7rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(\"/content/gdrive/MyDrive/Third_Model_75Dutch_25German\")"
      ],
      "metadata": {
        "id": "qd6IkWwud_DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "DrUN1OfBO2in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will have to do the \"important\" part, which is preprocess the audio files in our data:"
      ],
      "metadata": {
        "id": "xXlTBbfSO6_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_train[0][\"audio\"]"
      ],
      "metadata": {
        "id": "6UcyLkZlCyKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above we can see that the audio data is loaded with a sampling rate of 48kHZ whereas 16kHz are expected by the model. We can set the audio feature to the correct sampling rate by making use of cast_column:"
      ],
      "metadata": {
        "id": "2LtM7vcKPQmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "import torchaudio\n",
        "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "common_voice_train_nl = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "common_voice_test_nl = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "metadata": {
        "id": "Y7AIiGKbC0pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_train[0][\"audio\"]"
      ],
      "metadata": {
        "id": "zWq6hXP8DFBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y-7nGpIOQHHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now it is correctly cast, now lets listen to a couple audio pieces & check if the correspond to their 'sentence' given."
      ],
      "metadata": {
        "id": "CPEViXqlPSkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import IPython.display as ipd\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def show_random_sentence_with_audio(dataset):\n",
        "    pick = random.randint(0, len(dataset) - 1)  # Pick one random index\n",
        "\n",
        "    sentence = dataset[pick][\"sentence\"]\n",
        "    audio_data = dataset[pick][\"audio\"][\"array\"]\n",
        "    sample_rate = dataset[pick][\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    # Convert the audio data into an embedded player\n",
        "    audio_html = ipd.Audio(data=audio_data, rate=sample_rate)._repr_html_()\n",
        "\n",
        "    # Generate HTML output\n",
        "    html = f\"\"\"\n",
        "    <p><strong>Sentence:</strong> {sentence}</p>\n",
        "    {audio_html}\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(html))\n",
        "\n",
        "# Call the function with your dataset\n",
        "show_random_sentence_with_audio(common_voice_train)"
      ],
      "metadata": {
        "id": "riabel6BQElD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next you can check if each audio sample is 1 one dimensional array, with its transcript & its sampling rate:"
      ],
      "metadata": {
        "id": "KYrxJ3XhRd8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_int = random.randint(0, len(common_voice_train)-1)\n",
        "\n",
        "print(\"Target text:\", common_voice_train[rand_int][\"sentence\"])\n",
        "print(\"Input array shape:\", common_voice_train[rand_int][\"audio\"][\"array\"].shape)\n",
        "print(\"Sampling rate:\", common_voice_train[rand_int][\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "bVbGgkuRDKkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can leverage Wav2Vec2Processor to process the data to the format expected by Wav2Vec2ForCTC for training. We will again make use of the map(...) function.\n",
        "\n",
        "First, we load and resample the audio data, simply by calling batch[\"audio\"]. Second, we extract the input_values from the loaded audio file. Third, we encode the transcriptions to label ids.\n",
        "\n",
        "Note: This mapping function is a good example of how the Wav2Vec2Processor class should be used. In \"normal\" context, calling processor(...) is redirected to Wav2Vec2FeatureExtractor's call method. When wrapping the processor into the as_target_processor context, however, the same method is redirected to Wav2Vec2CTCTokenizer's call method. For more information please check the docs."
      ],
      "metadata": {
        "id": "OO5tDsrRR1u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    try:\n",
        "        audio = batch[\"audio\"]\n",
        "        batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "        with processor.as_target_processor():\n",
        "            batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
        "        return batch\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {batch}\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        raise e  # Re-raise the exception for debugging"
      ],
      "metadata": {
        "id": "AH7_IiogDOaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply the data preparation function to all examples."
      ],
      "metadata": {
        "id": "32CWX2jPSCt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, num_proc=1)\n",
        "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, num_proc=1)\n"
      ],
      "metadata": {
        "id": "8NTUUvFSDQYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "wBC0r3rWSG9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is processed so that we are ready to start setting up the training pipeline. We will make use of HuggingFace's Trainer for which we essentially need to do the following:\n",
        "\n",
        "* Define a data collator. In contrast to most NLP models, XLSR-Wav2Vec2 has a much larger input length than output length. E.g., a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning XLSR-Wav2Vec2 requires a special padding data collator, which we will define below\n",
        "\n",
        "* Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a compute_metrics function accordingly\n",
        "\n",
        "* Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
        "\n",
        "* Define the training configuration.\n",
        "\n",
        "After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech."
      ],
      "metadata": {
        "id": "_Y8DaBzoSUJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the trainer"
      ],
      "metadata": {
        "id": "wGshigs-SsVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by defining the data collator. In contrast to the common data collators, this data collator treats the input_values and labels differently and thus applies to separate padding functions on them (again making use of XLSR-Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function. Analogous to the common data collators, the padding tokens in the labels with -100 so that those tokens are not taken into account when computing the loss."
      ],
      "metadata": {
        "id": "JbVBlR8bSyFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "7qkqZee-DghK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "metadata": {
        "id": "KGdaLMnCDwJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wer_metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "b_dLs2z8DyDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qZU5p-deqB"
      },
      "source": [
        "The model will return a sequence of logit vectors:\n",
        "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
        "\n",
        "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "8z7Q_8rKEAv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can load the pretrained XLSR-Wav2Vec2 checkpoint. The tokenizer's pad_token_id must be to define the model's pad_token_id or in the case of Wav2Vec2ForCTC also CTC's blank token  2 . To save GPU memory, we enable PyTorch's gradient checkpointing and also set the loss reduction to \"mean\"."
      ],
      "metadata": {
        "id": "ZGUV7-DTTKL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\",\n",
        "    attention_dropout=0.05,  # Lower dropout due to smaller dataset\n",
        "    hidden_dropout=0.05,  # Reduce overfitting risk\n",
        "    feat_proj_dropout=0.0,  # Keep feature projection dropout at 0\n",
        "    mask_time_prob=0.08,  # Increase slightly for better generalization\n",
        "    layerdrop=0.1,  # Keep default, balances stability & regularization\n",
        "    ctc_loss_reduction=\"mean\",  # Standard setting\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,  # Correct padding\n",
        "    vocab_size=len(processor.tokenizer)  # Ensure correct vocabulary size\n",
        ")"
      ],
      "metadata": {
        "id": "pS04uU_iENmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DwR3XLSzGDD"
      },
      "source": [
        "The first component of XLSR-Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore.\n",
        "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.freeze_feature_extractor()"
      ],
      "metadata": {
        "id": "7mIf3gX7ERDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, let's enable gradient checkpointing to save some memory."
      ],
      "metadata": {
        "id": "VLS5oHvLT2Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "ZrDqPAlvER8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4aGhQM0K-D"
      },
      "source": [
        "In a final step, we define all parameters related to training.\n",
        "To give more explanation on some of the parameters:\n",
        "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
        "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.\n",
        "\n",
        "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
        "\n",
        "**Note**: depending on where you wish to save your model, un-comment the first or second option"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/gdrive/MyDrive/Third_Model_25Dutch_75German\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  save_steps=500,\n",
        "  eval_steps=500,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "J2xQoKv2ETq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ],
      "metadata": {
        "id": "DFNna7trU3rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=common_voice_train,\n",
        "    eval_dataset=common_voice_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "IGBpfwDvEaEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1f7d6b2cb727a17a1542ec2339cd444b97654961"
      ],
      "metadata": {
        "id": "HyKZydySVzEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2Ol9_zknEmT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lVjibo6Wjlbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (right mouse click -> inspect -> Console tab and insert code)."
      ],
      "metadata": {
        "id": "3A1wDcVoVJBU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYYAvgkW4P0m"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./Third_Model_25Dutch_75German\")\n",
        "processor.save_pretrained(\"./Third_Processor_25Dutch_75German\")"
      ],
      "metadata": {
        "id": "qnpKrel-GO_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FOnjiyuYeuEE"
      }
    }
  ]
}